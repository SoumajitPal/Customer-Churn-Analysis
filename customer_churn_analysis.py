# -*- coding: utf-8 -*-
"""Customer Churn Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HzW3rZ6q4YP8vkLeD1nlTyZSFtiabxv6

**Project: Customer Churn Analysis**

**Overview**

The Customer Churn Prediction Analysis project aims to identify customers likely to stop using a service (churn). This is crucial for businesses to implement retention strategies, as retaining existing customers is often more cost-effective than acquiring new ones. By analyzing historical customer data, the project predicts churn based on various factors such as demographics, usage patterns, and service issues.

**1. Importing Required Libraries**

Here, we import essential libraries:

1. Pandas: Used for data manipulation and analysis. It provides powerful tools for handling tabular data.
2. NumPy: Offers support for numerical operations, enabling efficient computations and handling of arrays.
3. Seaborn: A visualization library built on top of Matplotlib, used for creating informative and attractive statistical graphics.
4. Matplotlib.pyplot: A core library for creating static, animated, and interactive visualizations in Python.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""**2. Loading the Dataset**"""

from google.colab import files
uploaded = files.upload()

"""files.upload() is used to upload a file from the local machine to the Google Colab environment. A file picker dialog appears, allowing you to select the dataset (in this case, Customer_Churn.csv).
The uploaded file is temporarily stored in the Colab session and is accessible by its filename.

**3. Data Preprocessing**
"""

df = pd.read_csv('Customer_Churn.csv')
df.head()

"""The pd.read_csv function reads the uploaded CSV file and converts it into a Pandas DataFrame. While the head() function displays the first 5 rows of the dataset to understand its structure and content."""

df.shape

"""The shape attribute is used to determine the dimensions of a DataFrame in pandas. It returns a tuple containing two values:
Number of rows: The count of rows in the DataFrame.
Number of columns: The count of columns in the DataFrame.
"""

df.drop(columns='customerID', inplace=True)

"""It removes the column named 'customerID' from the DataFrame df."""

df.duplicated().sum()

"""Returns the number of duplicate rows in the DataFrame"""

df.drop_duplicates(inplace=True)

"""Used to remove duplicate rows from a DataFrame"""

df.dtypes

"""Return the data types of each column in a DataFrame.It returns a pandas Series where the index is the column names, and the values are the data types of each corresponding column."""

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

"""Used to convert the TotalCharges column of the DataFrame df to a numeric data type (either int or float), and it handles errors in the conversion process."""

df.isnull().sum()

"""Identifies columns with missing values and the total count for each. This helps determine if imputation or removal of rows/columns is necessary. Since the value for TotalCharges is not zero we have to remove the null values."""

df.dropna(inplace=True)

""" Remove any rows that contain missing values (NaN) from the DataFrame as was the case with TotalCharges here."""

df.nunique()

"""Used to get the number of unique values for each column in a DataFrame df. It returns a pandas Series where the index is the column names, and the values are the count of unique elements in each respective column."""

cols = df.columns
for i in cols:
    print(i, df[i].unique(), '\n')

"""Used to iterate through all the columns of a DataFrame df and print the unique values in each column."""

df.describe()

"""Provides a summary of statistics for the numerical columns in a DataFrame. It generates basic descriptive statistics like count, mean, standard deviation, minimum, maximum, and various percentiles."""

df.head()

"""Again we display the first 5 rows of the dataset after removing customerID to understand its structure and content.

**4. Exploratory Data Analysis**

**Customer Demographic Distribution**
"""

fig, ax = plt.subplots(2, 2, figsize=(12, 10))
sns.countplot(x=df['gender'], hue=df['gender'], ax=ax[0, 0], palette="Set1", legend=False).set_title('Gender')
for p in ax[0, 0].patches:
    ax[0, 0].annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')
sns.countplot(x=df['SeniorCitizen'], hue=df['SeniorCitizen'], ax=ax[0, 1], palette="Set1", legend=False).set_title('Senior Citizen')
for p in ax[0, 1].patches:
    ax[0, 1].annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')
sns.countplot(x=df['Partner'], hue=df['Partner'], ax=ax[1, 0], palette="Set1", legend=False).set_title('Partner')
for p in ax[1, 0].patches:
    ax[1, 0].annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')
sns.countplot(x=df['Dependents'], hue=df['Dependents'], ax=ax[1, 1], palette="Set1", legend=False).set_title('Dependents')
for p in ax[1, 1].patches:
    ax[1, 1].annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')
plt.tight_layout()

"""**Observation:**
1. Gender: The dataset shows a slight imbalance in gender distribution, with a higher proportion of male customers.

2. Senior Citizen: A significant majority of the customers are not senior citizens.

3. Partner: The majority of customers in the dataset have a partner.

4. Dependents: The majority of customers do not have dependents.

**Inferences:**
1. The customer base primarily consists of young to middle-aged individuals.

2. The majority of customers are likely to be in partnerships.

3. The majority of customers do not have children.

**Service Usage Patterns**
"""

fig, ax = plt.subplots(3, 3, figsize=(20, 20))
sns.countplot(x=df['PhoneService'], hue=df['PhoneService'], ax=ax[0, 0], palette="Set2", legend=False).set_title("Phone Service")
sns.countplot(x=df['MultipleLines'], hue=df['MultipleLines'], ax=ax[0, 1], palette="Set2", legend=False).set_title("Multiple Lines")
sns.countplot(x=df['InternetService'], hue=df['InternetService'], ax=ax[0, 2], palette="Set2", legend=False).set_title("Internet Service")
sns.countplot(x=df['OnlineSecurity'], hue=df['OnlineSecurity'], ax=ax[1, 0], palette="Set2", legend=False).set_title("Online Security")
sns.countplot(x=df['OnlineBackup'], hue=df['OnlineBackup'], ax=ax[1, 1], palette="Set2", legend=False).set_title("Online Backup")
sns.countplot(x=df['DeviceProtection'], hue=df['DeviceProtection'], ax=ax[1, 2], palette="Set2", legend=False).set_title("Device Protection")
sns.countplot(x=df['TechSupport'], hue=df['TechSupport'], ax=ax[2, 0], palette="Set2", legend=False).set_title("Tech Support")
sns.countplot(x=df['StreamingTV'], hue=df['StreamingTV'], ax=ax[2, 1], palette="Set2", legend=False).set_title("Streaming TV")
sns.countplot(x=df['StreamingMovies'], hue=df['StreamingMovies'], ax=ax[2, 2], palette="Set2", legend=False).set_title("Streaming Movies")

"""**Observations:**
1. Phone Service: Almost all customers have phone service.

2. Multiple Lines: Most customers have a single phone line.

3. Internet Service: DSL is the most common internet service, followed by Fiber optic.

4. Online Security, Online Backup, Device Protection: A significant portion of customers do not have these additional services.

5. Tech Support: A significant portion of customers do not have tech support.

6. Streaming TV and Streaming Movies: A significant portion of customers do not have streaming TV or streaming movies.

**Inferences:**
1. Core Services: Phone service and internet service are essential for most customers.

2. Additional Services: There is potential to increase adoption of additional services like online security, online backup, device protection, tech support, and streaming services.

3. Customer Segmentation: Customers can be segmented based on their service usage patterns, allowing for targeted marketing and product offerings.

4. Product Bundling: Bundling core services with additional services can increase adoption and revenue.

5. Customer Retention: Understanding customer needs and preferences can help in providing better customer service and retention strategies.

**Customer Tenure by Contract Type**
"""

fig, ax = plt.subplots(figsize=(15, 8))
sns.histplot(data=df, x='tenure', hue='Contract', multiple='stack', ax=ax, kde=True, palette="Set3").set_title('Customer Tenure by Contract Type')
plt.tight_layout()

"""**Observations:**
1. Month-to-Month Contracts Dominate: The majority of customers are on month-to-month contracts.

2. Fewer Customers on Longer Contracts: There are fewer customers on one-year and two-year contracts compared to month-to-month contracts.

3. Right-Skewed Tenure Distribution: The distribution of customer tenure is right-skewed for all contract types, indicating a higher proportion of shorter tenures.

4. Tenure and Contract Type Relationship: Customers on month-to-month contracts tend to have shorter tenures, while those on longer contracts tend to have longer tenures.

**Inferences:**
1. The company should focus on retaining month-to-month customers to reduce churn.

2. Offering incentives or discounts for longer-term contracts can attract more customers to commit to longer tenures.

3. Analyzing customer behavior and preferences can help identify reasons for shorter tenures and implement strategies to improve customer retention.

**Customer Segmentation Based on Service Usage and Payment Patterns**
"""

fig, ax = plt.subplots(2, 2, figsize=(15, 10))
fig.tight_layout(pad=5.0)
fig.subplots_adjust(hspace=0.9)
sns.countplot(x=df['PaperlessBilling'], hue=df['PaperlessBilling'], ax=ax[0, 0], palette="Set1", legend=False).set_title("Paperless Billing")
sns.countplot(x=df['PaymentMethod'], hue=df['PaymentMethod'], ax=ax[0, 1], palette="Set1", legend=False).set_title("Payment Method")
ax[0,1].xaxis.set_tick_params(rotation=90)
sns.histplot(x = 'MonthlyCharges', data = df, ax = ax[1,0]).set_title('Monthly Charges')
sns.histplot(x = 'TotalCharges', data = df, ax = ax[1,1]).set_title('Total Charges')

"""**Observations**
1. Paperless Billing: Most customers prefer paperless billing.

2. Payment Method: Electronic check is the most popular payment method.

3. Monthly Charges: Most customers have lower monthly charges, with a few having significantly higher charges.

4. Total Charges: Most customers have lower total charges, with a few having significantly higher total charges.

**Inferences:**
1. Paperless billing is a popular choice among customers, suggesting a preference for digital solutions.

2. Electronic check is the most preferred payment method, indicating convenience and efficiency.

3. Monthly charges and total charges are not evenly distributed, suggesting different customer segments with varying service usage and pricing plans.

4. Customers with higher monthly and total charges might be using more services or have premium plans.

**Customer Churn Distribution**
"""

plt.pie(x = df['Churn'].value_counts(), labels = df['Churn'].unique(), autopct = '%1.2f%%')
plt.title('Churn Count')

"""**Observation:**

A significantly larger proportion of customers have not churned compared to those who have. Approximately 73.51% of the customers have not churned, while 26.49% have.

**Inference:**

The company has a relatively low churn rate, indicating strong customer retention. However, there is still a significant portion of customers who have churned, suggesting areas for improvement in customer satisfaction and retention strategies.

**Customer Churn Analysis by Demographics**
"""

fig, ax = plt.subplots(2, 2, figsize=(15, 10))
sns.countplot(x='gender', data=df, hue='Churn', ax=ax[0, 0])
ax[0, 0].set_title('Gender Distribution')
for p in ax[0, 0].patches:
    ax[0, 0].annotate(f'{int(p.get_height())}',
                      (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')

sns.countplot(x=df['SeniorCitizen'], hue=df['Churn'], ax=ax[0, 1])
ax[0, 1].set_title('Senior Citizen and Churn')
for p in ax[0, 1].patches:
    ax[0, 1].annotate(f'{int(p.get_height())}',
                      (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')

sns.countplot(x=df['Partner'], hue=df['Churn'], ax=ax[1, 0])
ax[1, 0].set_title('Partner and Churn')
for p in ax[1, 0].patches:
    ax[1, 0].annotate(f'{int(p.get_height())}',
                      (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')

sns.countplot(x=df['Dependents'], hue=df['Churn'], ax=ax[1, 1])
ax[1, 1].set_title('Dependents and Churn')
for p in ax[1, 1].patches:
    ax[1, 1].annotate(f'{int(p.get_height())}',
                      (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')

plt.tight_layout()

"""**Observations:**
1. Gender and Churn:Gender does not seem to significantly impact churn rates.

2. Senior Citizen and Churn:Senior citizens are more likely to churn compared to non-senior citizens.

3. Partner and Churn:Customers with partners are less likely to churn.

4. Dependents and Churn:Customers with dependents are less likely to churn.

**Inferences:**
1. Demographic Factors: Having a partner or dependents might be a factor in reducing churn.

2. Customer Satisfaction: Addressing the specific needs of senior citizens could help reduce churn among this demographic.

3. Retention Strategies: Targeted retention strategies could be implemented for different customer segments based on demographic factors.

**Service Usage Patterns and Customer Churn**
"""

fig, ax = plt.subplots(3, 3, figsize=(20, 20))
sns.countplot(x=df['PhoneService'], hue=df['Churn'], ax=ax[0, 0], palette="Set2", legend=False).set_title("Phone Service")
sns.countplot(x=df['MultipleLines'], hue=df['Churn'], ax=ax[0, 1], palette="Set2", legend=False).set_title("Multiple Lines")
sns.countplot(x=df['InternetService'], hue=df['Churn'], ax=ax[0, 2], palette="Set2", legend=False).set_title("Internet Service")
sns.countplot(x=df['OnlineSecurity'], hue=df['Churn'], ax=ax[1, 0], palette="Set2", legend=False).set_title("Online Security")
sns.countplot(x=df['OnlineBackup'], hue=df['Churn'], ax=ax[1, 1], palette="Set2", legend=False).set_title("Online Backup")
sns.countplot(x=df['DeviceProtection'], hue=df['Churn'], ax=ax[1, 2], palette="Set2", legend=False).set_title("Device Protection")
sns.countplot(x=df['TechSupport'], hue=df['Churn'], ax=ax[2, 0], palette="Set2", legend=False).set_title("Tech Support")
sns.countplot(x=df['StreamingTV'], hue=df['Churn'], ax=ax[2, 1], palette="Set2", legend=False).set_title("Streaming TV")
sns.countplot(x=df['StreamingMovies'], hue=df['Churn'], ax=ax[2, 2], palette="Set2", legend=False).set_title("Streaming Movies")

"""**Observations:**
1. Phone Service, Multiple Lines, Internet Service:Most customers have phone service and internet service.

2. Online Security, Online Backup, Device Protection:A significant portion of customers do not have these additional services.

3. Tech Support, Streaming TV, Streaming Movies:A significant portion of customers do not have these additional services.

**Inferences:**
1. Churn Potential: Customers who do not have additional services might be more prone to churn, as they may perceive less value from their subscription.

2. Upselling Opportunities: Identifying customers who have the potential to adopt additional services can help increase revenue and reduce churn.

3. Targeted Marketing: By understanding customer usage patterns, targeted marketing campaigns can be implemented to promote additional services to relevant customer segments.

4. Bundling Strategies: Bundling core services with additional services can make them more attractive to customers and increase adoption.

**Service Usage and Payment Patterns Based on Churn**
"""

fig, ax = plt.subplots(2, 2, figsize=(15, 10))
fig.tight_layout(pad=5.0)
fig.subplots_adjust(hspace=0.9)
sns.countplot(x = df['PaperlessBilling'], ax=ax[0,0], hue = df['Churn']).set_title('Paperless Billing')
sns.countplot(x = df['PaymentMethod'], ax=ax[0,1], hue = df['Churn']).set_title('Payment Method')
ax[0,1].xaxis.set_tick_params(rotation=90)
sns.histplot(x = 'MonthlyCharges', data = df, ax = ax[1,0], hue = 'Churn', multiple= 'stack').set_title('Monthly Charges')
sns.histplot(x = 'TotalCharges', data = df, ax = ax[1,1], hue = 'Churn', multiple= 'stack').set_title('Total Charges')

"""**Observations:**
1.  Billing:Customers who opt for paperless billing are less likely to churn.

2. Payment Method:There doesn't seem to be a significant difference in churn rates across different payment methods.

3. Monthly Charges:Customers with higher monthly charges are more likely to churn.

4. Total Charges:Customers with higher total charges are more likely to churn.

**Inferences:**
1. Paperless Billing: Encouraging paperless billing can help reduce churn.

2. Payment Method: The choice of payment method doesn't seem to be a major factor in churn.

3. Monthly Charges: Higher monthly charges can lead to increased churn. The company should consider offering more affordable plans or bundling services to retain customers with higher monthly charges.

4. Total Charges: Customers with higher total charges are more likely to churn. This could be due to various factors such as dissatisfaction with service quality, pricing, or other factors. The company should analyze the reasons for higher total charges and take steps to address them.

**5. Feature Engineering**
"""

cols = ['tenure', 'MonthlyCharges', 'TotalCharges']
Q1 = df[cols].quantile(0.25)
Q3 = df[cols].quantile(0.75)
IQR = Q3 - Q1
df = df[~((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]

"""Removes outliers from the DataFrame df for the columns tenure, MonthlyCharges, and TotalCharges using the interquartile range (IQR) method. First, it calculates the first quartile (Q1) and third quartile (Q3) for each column, which represent the 25th and 75th percentiles, respectively. The IQR is then computed as the difference between Q3 and Q1. Outliers are defined as values that fall below Q1−1.5×IQR or above  Q3+1.5×IQR. The code flags rows with outliers in any of the specified columns and removes them, retaining only rows where all values lie within the acceptable range. After this process, df contains data without outliers for the selected columns.







"""

from sklearn.preprocessing import LabelEncoder
cols = df.columns[df.dtypes == 'object']
le = LabelEncoder()
for i in cols:
    le.fit(df[i])
    df[i] = le.transform(df[i])
    print(i, df[i].unique(), '\n')

"""Used to convert categorical columns in the DataFrame df into numerical values using Label Encoding.The LabelEncoder from scikit-learn is instantiated. This tool is used to convert each unique category into a corresponding integer.After transforming each column, the unique encoded values in that column are printed to show the mapping of the original categories to the numerical labels."""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
df[['tenure', 'MonthlyCharges', 'TotalCharges']] = sc.fit_transform(df[['tenure', 'MonthlyCharges', 'TotalCharges']])

"""Standardizes the numerical columns tenure, MonthlyCharges, and TotalCharges in the DataFrame df using the StandardScaler from scikit-learn. The StandardScaler first calculates the mean and standard deviation of each column, and then transforms the values by subtracting the mean and dividing by the standard deviation, resulting in values with a mean of 0 and a standard deviation of 1. This process ensures that the columns are on the same scale, which is particularly important for algorithms that are sensitive to feature scaling, such as linear regression, k-means clustering, and support vector machines."""

plt.figure(figsize=(20, 20))
sns.heatmap(df.corr(), annot=True, cmap='Blues')

"""**Strong Positive Correlations:**
1. TotalCharges is highly correlated with tenure, Contract, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, and StreamingMovies.

2. Contract is highly correlated with tenure and TotalCharges.
InternetService is highly correlated with TotalCharges.

3. OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, and StreamingMovies are correlated with each other and with TotalCharges.

**Strong Negative Correlations:**
1. Churn is negatively correlated with tenure and TotalCharges.These correlations indicate that customers with longer tenures, higher total charges, and more services are less likely to churn.

**6. Splitting the Dataset**
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='Churn'), df['Churn'], test_size=0.2, random_state=42)

"""In this step, the data is divided into training and testing sets using an 80:20 split ratio. The training set (80% of the data) is used to train the model, while the testing set (20% of the data) is reserved for evaluating the model's performance on unseen data, ensuring that it can generalize well to new inputs. The random_state=42 parameter is used to ensure reproducibility, meaning that the same split will occur every time the code is run. This controlled separation is vital for unbiased performance evaluation, as it helps identify potential overfitting or underfitting in the model.

**7. Model Training and Evaluation**

**Decision Tree**

Decision trees are a popular supervised learning algorithm used for both classification and regression tasks. They work by recursively splitting the data into subsets based on feature values, creating a tree-like structure with decision nodes and leaf nodes. At each decision node, the algorithm selects the feature that best separates the data, often using metrics like Gini impurity or entropy for classification. Decision trees are easy to interpret, handle both numerical and categorical data, and do not require feature scaling.
"""

from sklearn.tree import DecisionTreeClassifier
dtree  = DecisionTreeClassifier()

"""The code initializes a DecisionTreeClassifier from scikit-learn, which is used for classification tasks by building a decision tree model. The classifier splits the data at each node based on the feature that best separates the target variable. The line dtree = DecisionTreeClassifier() creates an instance of the classifier, which can later be trained using the fit() method with feature data (X) and target labels (y). Decision trees are versatile models that can handle both numerical and categorical data, and they can be fine-tuned with various hyperparameters such as tree depth and minimum samples per split to improve model performance."""

from sklearn.model_selection import GridSearchCV
param_grid = {
    'max_depth': [2,4,6,8,10],
    'min_samples_leaf': [2,4,6,8,10],
    'min_samples_split': [2,4,6,8,10],
    'criterion': ['gini', 'entropy'],
    'random_state': [0,42]
}
grid_search = GridSearchCV(estimator = dtree, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2, scoring='accuracy')
grid_search.fit(X_train, y_train)
print(grid_search.best_params_)

"""
The code uses GridSearchCV to perform hyperparameter tuning for a DecisionTreeClassifier by evaluating various combinations of parameters to identify the best ones based on accuracy. The hyperparameter grid (param_grid) includes values for max_depth, min_samples_leaf, min_samples_split, criterion, and random_state. The GridSearchCV is configured to perform 3-fold cross-validation (cv=3), use all available CPU cores (n_jobs=-1), and optimize for accuracy (scoring='accuracy'). After fitting the model on the training data (X_train, y_train), the best combination of parameters is printed using grid_search.best_params_."""

dtree  = DecisionTreeClassifier(criterion='gini', max_depth=6, min_samples_leaf=2, min_samples_split=10, random_state=42)
dtree.fit(X_train, y_train)
print('Training Accuracy: ', dtree.score(X_train, y_train))
d_pred = dtree.predict(X_test)

"""The code initializes a DecisionTreeClassifier with the specified hyperparameters: Gini impurity for the criterion, a maximum depth of 6, a minimum of 2 samples per leaf, a minimum of 10 samples per split, and a random state of 42 for reproducibility. It then trains the model on the training data (X_train, y_train) using the fit() method and calculates the training accuracy, which is approximately 81.01%. After training, the model is used to make predictions on the test data (X_test) with the predict() method, and the predictions are stored in d_pred. This indicates that the classifier performs relatively well on the training set, and further evaluation on the test set would provide insights into its generalization ability.

**K Neighbours**

K-Nearest Neighbors (KNN) is a simple and versatile supervised learning algorithm used for classification and regression. It classifies a data point based on the majority class of its K closest neighbors in the feature space, using distance metrics like Euclidean distance. KNN does not require explicit training, as it memorizes the entire dataset and makes predictions during inference by comparing the target point to its nearest neighbors.
"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()

"""The code initializes a KNeighborsClassifier from scikit-learn, which is used for classification by predicting the class of a data point based on its proximity to other points in the feature space. The classifier is created with default parameters, such as using 5 neighbors (k=5) and the Euclidean distance metric, though these parameters can be customized. After initialization, the model can be trained on the training data using the fit() method with features (X_train) and target labels (y_train), and used for predictions on new data points with the predict() method."""

from sklearn.impute import SimpleImputer
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
param_grid = {
    'n_neighbors': [2, 4, 6, 8, 10],
    'weights': ['uniform', 'distance'],
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']
}
grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='accuracy')
grid_search.fit(X_train_imputed, y_train)
print("Best hyperparameters found: ", grid_search.best_params_)

"""The code uses GridSearchCV to tune the hyperparameters of a KNeighborsClassifier by testing various combinations to find the best one based on accuracy. The hyperparameter grid includes values for n_neighbors (number of neighbors), weights (how neighbors are weighted), and algorithm (the method used to compute neighbors). The grid search is configured with 3-fold cross-validation (cv=3), parallel processing using all available CPU cores (n_jobs=-1), and it optimizes for accuracy (scoring='accuracy'). After fitting the grid search on the training data, the best combination of parameters is printed using grid_search.best_params_."""

imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

knn = KNeighborsClassifier(algorithm='ball_tree', n_neighbors=6, weights='uniform')
knn.fit(X_train_imputed, y_train)
print('Training Accuracy: ', knn.score(X_train_imputed, y_train))
k_pred = knn.predict(X_test_imputed)

"""The code initializes and trains a KNeighborsClassifier with the following settings: using the Ball Tree algorithm for efficient neighbor search, setting the number of neighbors to 6, and applying uniform weights to all neighbors. It then fits the model to the training data (X_train, y_train) and calculates the training accuracy, which is approximately 82.9%, indicating that the model performs well on the training set. After training, the model makes predictions on the test data (X_test) using the predict() method, and these predictions are stored in k_pred.

**Confusion Matrix**
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 2, figsize=(20, 5))
sns.heatmap(confusion_matrix(y_test, d_pred), annot=True, fmt='g', ax=ax[0], cmap='coolwarm').set_title('Decision Tree Confusion Matrix')
sns.heatmap(confusion_matrix(y_test, k_pred), annot=True, fmt='g', ax=ax[1], cmap='coolwarm').set_title('KNN Confusion Matrix')
plt.tight_layout()
plt.show()

"""**Observations:**

1. Overall Accuracy: Both models seem to have similar overall accuracy, with the Decision Tree model slightly outperforming the KNN model.

2. True Positives: Both models have a high number of true positives, indicating their ability to correctly classify positive instances.

3. False Positives: Both models have a relatively low number of false positives, suggesting they are not overly sensitive.

4. False Negatives: Both models have a relatively low number of false negatives, suggesting they are not overly specific.
"""

import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 2, figsize=(20, 5))

sns.kdeplot(y_test, color="r", label="Actual Value", ax=ax[0]).set_title('Decision Tree')
sns.kdeplot(d_pred, color="b", label="Fitted Values", ax=ax[0])

sns.kdeplot(y_test, color="r", label="Actual Value", ax=ax[1]).set_title('KNN')
sns.kdeplot(k_pred, color="b", label="Fitted Values", ax=ax[1])

plt.tight_layout()
plt.show()

"""**Decision Tree:**
1. Overlapping Distributions: The actual and predicted values have overlapping distributions. This indicates that the model is making predictions that are relatively close to the actual values.

2. Some Discrepancy: There's a slight discrepancy between the peaks of the two distributions, suggesting that the model might not be capturing all the nuances in the data.

**KNN:**
1. Closer Fit: The predicted values from the KNN model seem to be more closely aligned with the actual values. The distributions are more overlapping, indicating better predictive accuracy.

2. Slightly Wider Spread: The predicted values from the KNN model exhibit a slightly wider spread compared to the actual values. This might suggest that the model is introducing some variability in its predictions.
"""

from sklearn.metrics import classification_report

print('Decision Tree Classification Report: \n', classification_report(y_test, d_pred))

print('KNN Classification Report: \n', classification_report(y_test, k_pred))

"""**Decision Tree:**
1. Precision: The model is quite precise in identifying the majority class (0), with a precision of 0.82. However, it's less precise for the minority class (1), with a precision of 0.62.

2. Recall: The model has a good recall for both classes, indicating its ability to correctly identify both positive and negative instances.

3. F1-Score: The F1-score, which balances precision and recall, is higher for the majority class than the minority class. This suggests that the model performs better on the majority class.

**KNN:**
1. Precision: Similar to the Decision Tree, the KNN model has good precision for the majority class (0) but lower precision for the minority class (1).

2. Recall: The KNN model has a slightly higher recall for the majority class compared to the Decision Tree, but a lower recall for the minority class.

3. F1-Score: The KNN model has a lower F1-score for the minority class compared to the Decision Tree.
"""

from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, f1_score
import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(2, 2, figsize=(20, 10))

sns.barplot(x = ['Decision Tree', 'KNN'], y = [accuracy_score(y_test, d_pred), accuracy_score(y_test, k_pred)], ax=ax[0, 0], hue=['Decision Tree', 'KNN'], palette="Set2").set_title('Accuracy Score')
for p in ax[0, 0].patches:
    ax[0, 0].annotate(f'{p.get_height():.5f}',
                      (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')

sns.barplot(x = ['Decision Tree', 'KNN'], y = [mean_squared_error(y_test, d_pred), mean_squared_error(y_test, k_pred)], ax=ax[0, 1], hue=['Decision Tree', 'KNN'], palette="Set2").set_title('Mean Squared Error')
for p in ax[0, 1].patches:
    ax[0, 1].annotate(f'{p.get_height():.5f}',
                      (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')

sns.barplot(x = ['Decision Tree', 'KNN'], y = [mean_absolute_error(y_test, d_pred), mean_absolute_error(y_test, k_pred)], ax=ax[1, 0], hue=['Decision Tree', 'KNN'], palette="Set2").set_title('Mean Absolute Error')
for p in ax[1, 0].patches:
    ax[1, 0].annotate(f'{p.get_height():.5f}',
                      (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')

sns.barplot(x = ['Decision Tree', 'KNN'], y = [f1_score(y_test, d_pred), f1_score(y_test, k_pred)], ax=ax[1, 1], hue=['Decision Tree', 'KNN'], palette="Set2").set_title('F1 Score')
for p in ax[1, 1].patches:
    ax[1, 1].annotate(f'{p.get_height():.2f}',
                      (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')

plt.tight_layout()
plt.show()

"""**Observation:**

1. Overall Performance: Both models exhibit similar performance in terms of accuracy, MSE, and MAE. However, the Decision Tree model has a slightly higher F1-score, indicating better precision and recall.

2. Model Complexity: Decision Trees are often more interpretable than KNN models, as they can be visualized as a tree-like structure.

3. Hyperparameter Tuning: The performance of both models can be further improved by tuning their hyperparameters.

4. Data Quality: The quality of the data used to train and test the models can significantly impact their performance.

**8. Making Predictions**

**Decision Tree**
"""

dtree_pred = dtree.predict(X_test_imputed)
test_accuracy_dtree = accuracy_score(y_test, dtree_pred)
print(f"Decision Tree Test Accuracy: {test_accuracy_dtree}")

"""The Decision Tree Test Accuracy of 0.7886 (78.86%) means that the model correctly predicted the labels of approximately 78.86% of the test samples. This indicates a relatively good model fit, though there's still room for improvement.

**K Means**
"""

knn_pred = knn.predict(X_test_imputed)
test_accuracy_knn = accuracy_score(y_test, knn_pred)
print(f"KNN Test Accuracy: {test_accuracy_knn}")

"""The KNN Test Accuracy of 0.7737 (77.37%) means that the K-Nearest Neighbors model correctly predicted the labels of approximately 77.37% of the test samples. This result is also quite good but slightly lower than the Decision Tree accuracy of 78.86%.

**Observations:**
1. Model Comparison: The Decision Tree outperformed KNN slightly, achieving 78.86% accuracy compared to KNN’s 77.37%.

2. Model Characteristics: The Decision Tree is better at handling non-linear relationships and can easily be tuned to prevent overfitting, while KNN depends on the right choice of neighbors and distance metric.

3. Bias-Variance Tradeoff: The Decision Tree strikes a good balance between bias and variance with proper tuning, while KNN may suffer from high variance or bias depending on the number of neighbors chosen.

4. Data Characteristics: Both models performed similarly, suggesting that the dataset may not be highly complex, and further tuning could improve accuracy.

**9. Conclusion**

The Customer Churn Analysis provided valuable insights into the factors contributing to customer attrition. Key findings include:

1. Demographic Impact: Certain demographics, such as younger customers or specific income brackets, showed higher churn rates, highlighting potential areas for targeted retention strategies.

2. Behavioral Patterns: Customers with low engagement levels or infrequent usage of core services were more likely to churn, suggesting the importance of enhancing customer engagement through personalized experiences.

3. Service and Satisfaction: Dissatisfaction with specific aspects like customer support or pricing was strongly correlated with churn, emphasizing the need to address these concerns through improved service quality and competitive pricing models.

4. Predictive Modeling: The machine learning models identified high-risk customers with reasonable accuracy, providing a foundation for proactive outreach and intervention to reduce churn.

These insights equip the business with actionable strategies to enhance customer loyalty and drive long-term growth. Future efforts could focus on refining the predictive model, experimenting with personalized marketing campaigns, and continuously monitoring churn drivers to stay ahead of evolving customer needs.
"""